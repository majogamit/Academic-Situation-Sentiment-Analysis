{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "clean-homeless",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-night",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "urban-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# For Data processing/cleaning\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import STOPWORDS\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import os\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "requested-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"stemmer\\TagalogStemmerPython\")\n",
    "\n",
    "# import TglStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "african-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"nlp_research_dataset_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tracked-promotion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aboriginal-ethics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-27 10:30:42+00:00</td>\n",
       "      <td>dprleanne</td>\n",
       "      <td>lunes nanaman bukas #academicbreaknow</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-27 09:02:47+00:00</td>\n",
       "      <td>ericakieraa</td>\n",
       "      <td>#AcademicBreakNow gusto q na tapusin Wednesday</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-25 13:30:42+00:00</td>\n",
       "      <td>qin_ina</td>\n",
       "      <td>super delay na ako sa tbw list ko #academicbre...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-24 12:46:22+00:00</td>\n",
       "      <td>_patreng_</td>\n",
       "      <td>#/academicbreaknow tsngina pagod na 'ko magpaypay</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-24 08:18:08+00:00</td>\n",
       "      <td>willowveewise</td>\n",
       "      <td>Pagoda ang accla, 4hours tulog gising 3:40am l...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date           user  \\\n",
       "0  2022-11-27 10:30:42+00:00      dprleanne   \n",
       "1  2022-11-27 09:02:47+00:00    ericakieraa   \n",
       "2  2022-11-25 13:30:42+00:00        qin_ina   \n",
       "3  2022-11-24 12:46:22+00:00      _patreng_   \n",
       "4  2022-11-24 08:18:08+00:00  willowveewise   \n",
       "\n",
       "                                                text  label  \n",
       "0              lunes nanaman bukas #academicbreaknow     -1  \n",
       "1     #AcademicBreakNow gusto q na tapusin Wednesday     -1  \n",
       "2  super delay na ako sa tbw list ko #academicbre...     -1  \n",
       "3  #/academicbreaknow tsngina pagod na 'ko magpaypay     -1  \n",
       "4  Pagoda ang accla, 4hours tulog gising 3:40am l...     -1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "facial-senior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date     False\n",
       "user     False\n",
       "text     False\n",
       "label    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "instructional-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = dataset.drop_duplicates(subset=[\"text\"], keep='first')\n",
    "# df2.shape\n",
    "df2 = dataset[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-trick",
   "metadata": {},
   "source": [
    "### Removing @names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spanish-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(text,pattern):\n",
    "    \n",
    "    # re.findall() finds the pattern i.e @user and puts it in a list for further task\n",
    "    r = re.findall(pattern,text)\n",
    "    \n",
    "    # re.sub() removes @user from the sentences in the dataset\n",
    "    for i in r:\n",
    "        text = re.sub(i,\"\",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minor-indiana",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7ce2c0ade8c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tidy_tweets'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_pattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"@[\\w]*\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "df2['tidy_tweets'] = np.vectorize(remove_pattern)(df2['text'], \"@[\\w]*\")\n",
    "\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-philip",
   "metadata": {},
   "source": [
    "###  Removing Punctuations, Numbers, and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['tidy_tweets'] = df2['tidy_tweets'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-mechanics",
   "metadata": {},
   "source": [
    "### Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []\n",
    "\n",
    "for index, row in df2.iterrows():\n",
    "    # Here we are filtering out all the words that contains link\n",
    "    words_without_links = [word for word in row.tidy_tweets.split() if 'http' not in word]\n",
    "    cleaned_tweets.append(' '.join(words_without_links))\n",
    "\n",
    "df2['tidy_tweets'] = cleaned_tweets\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-psychology",
   "metadata": {},
   "source": [
    "### Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_char(text):\n",
    "    clean_tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "    return clean_tweet\n",
    "df2['tidy_tweets']=df2['tidy_tweets'].apply(clean_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-eleven",
   "metadata": {},
   "source": [
    "### Remove rows with empty texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = df2[df2['tidy_tweets']!='']\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-andrew",
   "metadata": {},
   "source": [
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_df.drop_duplicates(subset=['tidy_tweets'], keep='first')\n",
    "# tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-oasis",
   "metadata": {},
   "source": [
    "### Reset Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.reset_index(drop=True)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-occasions",
   "metadata": {},
   "source": [
    "### Remove special characters again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['absolute_tidy_tweets'] = tweets_df['tidy_tweets'].str.replace(\"[^a-zA-Z# ]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-russell",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-functionality",
   "metadata": {},
   "source": [
    "# Remove English and Filipino Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_eng = nltk.corpus.stopwords.words('english')\n",
    "stopword_fil = [\"akin\",\"aking\",\"ako\",\"alin\",\"am\",\"amin\",\"aming\",\"ang\",\"ano\",\"anumang\",\"apat\",\"at\",\"atin\",\"ating\",\"ay\",\"bababa\",\"bago\",\"bakit\",\"bawat\",\"bilang\",\"dahil\",\"dalawa\",\"dapat\",\"din\",\"dito\",\"doon\",\"gagawin\",\"gayunman\",\"ginagawa\",\"ginawa\",\"ginawang\",\"gumawa\",\"gusto\",\"habang\",\"hanggang\",\"hindi\",\"huwag\",\"iba\",\"ibaba\",\"ibabaw\",\"ibig\",\"ikaw\",\"ilagay\",\"ilalim\",\"ilan\",\"inyong\",\"isa\",\"isang\",\"itaas\",\"ito\",\"iyo\",\"iyon\",\"iyong\",\"ka\",\"kahit\",\"kailangan\",\"kailanman\",\"kami\",\"kanila\",\"kanilang\",\"kanino\",\"kanya\",\"kanyang\",\"kapag\",\"kapwa\",\"karamihan\",\"katiyakan\",\"katulad\",\"kaya\",\"kaysa\",\"ko\",\"kong\",\"kulang\",\"kumuha\",\"kung\",\"laban\",\"lahat\",\"lamang\",\"likod\",\"lima\",\"maaari\",\"maaaring\",\"maging\",\"mahusay\",\"makita\",\"marami\",\"marapat\",\"masyado\",\"may\",\"mayroon\",\"mga\",\"minsan\",\"mismo\",\"mula\",\"muli\",\"na\",\"nabanggit\",\"naging\",\"nagkaroon\",\"nais\",\"nakita\",\"namin\",\"napaka\",\"narito\",\"nasaan\",\"ng\",\"ngayon\",\"ni\",\"nila\",\"nilang\",\"nito\",\"niya\",\"niyang\",\"noon\",\"o\",\"pa\",\"paano\",\"pababa\",\"paggawa\",\"pagitan\",\"pagkakaroon\",\"pagkatapos\",\"palabas\",\"pamamagitan\",\"panahon\",\"pangalawa\",\"para\",\"paraan\",\"pareho\",\"pataas\",\"pero\",\"pumunta\",\"pumupunta\",\"sa\",\"saan\",\"sabi\",\"sabihin\",\"sarili\",\"sila\",\"sino\",\"siya\",\"tatlo\",\"tayo\",\"tulad\",\"tungkol\",\"una\",\"walang\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopword_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopword_fil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-surge",
   "metadata": {},
   "source": [
    "### Remove english stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_eng]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_eng]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "tweets_df['stopped_tweets']=tweets_df['absolute_tidy_tweets'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-trash",
   "metadata": {},
   "source": [
    "### Remove Filipino Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_fil]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_fil]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text.lower()\n",
    "\n",
    "tweets_df['stopped_tweets']=tweets_df['stopped_tweets'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-african",
   "metadata": {},
   "source": [
    "## export to csv for backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('02_Data_wo_Stopwords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-warrior",
   "metadata": {},
   "source": [
    "## Label Sentiments Automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-terminal",
   "metadata": {},
   "source": [
    "Compare Textblob and NLTK Sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import TextBlob\n",
    "# from textblob.sentiments import NaiveBayesAnalyzer\n",
    "# from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "# def fetch_sentiment_using_textblob(text):\n",
    "#     analysis = TextBlob(text)\n",
    "#     return 'pos' if analysis.sentiment.polarity >= 0 else 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiments_using_textblob = tweets_df.text.apply(lambda tweet: fetch_sentiment_using_textblob(tweet))\n",
    "# tweets_df['sentiment'] = sentiments_using_textblob\n",
    "# pd.DataFrame(sentiments_using_textblob.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# def fetch_sentiment_using_SIA(text):\n",
    "#     sid = SentimentIntensityAnalyzer()\n",
    "#     polarity_scores = sid.polarity_scores(text)\n",
    "#     return 'neg' if polarity_scores['neg'] > polarity_scores['pos'] else 'pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiments_using_SIA = tweets_df.text.apply(lambda tweet: fetch_sentiment_using_SIA(tweet))\n",
    "# tweets_df['sentiment'] = sentiments_using_SIA\n",
    "# pd.DataFrame(sentiments_using_SIA.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_df.loc[tweets_df.sentiment == 'neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-timeline",
   "metadata": {},
   "source": [
    "### Export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('03_Data_with_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-ozone",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3196968d684371006099b3d55edeef8ed90365227a30deaef86e5d4aa8519be0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
