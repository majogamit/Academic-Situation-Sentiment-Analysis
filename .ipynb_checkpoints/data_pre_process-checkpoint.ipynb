{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "clean-homeless",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-night",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "urban-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# For Data processing/cleaning\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import STOPWORDS\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import os\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "requested-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"stemmer\\TagalogStemmerPython\")\n",
    "\n",
    "# import TglStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "african-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"All_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tracked-promotion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36701, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aboriginal-ethics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-29 04:15:58+00:00</td>\n",
       "      <td>maffyolfato</td>\n",
       "      <td>#academicbreaknow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-27 12:18:49+00:00</td>\n",
       "      <td>researcheff</td>\n",
       "      <td>Hello, #AcademicTwitter,\\n\\nüéôÔ∏è\"You need to hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-27 10:30:42+00:00</td>\n",
       "      <td>dprleanne</td>\n",
       "      <td>lunes nanaman bukas #academicbreaknow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-27 09:02:47+00:00</td>\n",
       "      <td>ericakieraa</td>\n",
       "      <td>#AcademicBreakNow gusto q na tapusin Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-25 13:30:42+00:00</td>\n",
       "      <td>qin_ina</td>\n",
       "      <td>super delay na ako sa tbw list ko #academicbre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date         user  \\\n",
       "0  2022-11-29 04:15:58+00:00  maffyolfato   \n",
       "1  2022-11-27 12:18:49+00:00  researcheff   \n",
       "2  2022-11-27 10:30:42+00:00    dprleanne   \n",
       "3  2022-11-27 09:02:47+00:00  ericakieraa   \n",
       "4  2022-11-25 13:30:42+00:00      qin_ina   \n",
       "\n",
       "                                                text  \n",
       "0                                  #academicbreaknow  \n",
       "1  Hello, #AcademicTwitter,\\n\\nüéôÔ∏è\"You need to hav...  \n",
       "2              lunes nanaman bukas #academicbreaknow  \n",
       "3     #AcademicBreakNow gusto q na tapusin Wednesday  \n",
       "4  super delay na ako sa tbw list ko #academicbre...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "facial-senior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date    False\n",
       "user    False\n",
       "text    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "instructional-frederick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29596, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = dataset.drop_duplicates(subset=[\"text\"], keep='first')\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-trick",
   "metadata": {},
   "source": [
    "### Removing @names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spanish-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(text,pattern):\n",
    "    \n",
    "    # re.findall() finds the pattern i.e @user and puts it in a list for further task\n",
    "    r = re.findall(pattern,text)\n",
    "    \n",
    "    # re.sub() removes @user from the sentences in the dataset\n",
    "    for i in r:\n",
    "        text = re.sub(i,\"\",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minor-indiana",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-7ce2c0ade8c3>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['tidy_tweets'] = np.vectorize(remove_pattern)(df2['text'], \"@[\\w]*\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>tidy_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-29 04:15:58+00:00</td>\n",
       "      <td>maffyolfato</td>\n",
       "      <td>#academicbreaknow</td>\n",
       "      <td>#academicbreaknow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-27 12:18:49+00:00</td>\n",
       "      <td>researcheff</td>\n",
       "      <td>Hello, #AcademicTwitter,\\n\\nüéôÔ∏è\"You need to hav...</td>\n",
       "      <td>Hello, #AcademicTwitter,\\n\\nüéôÔ∏è\"You need to hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-27 10:30:42+00:00</td>\n",
       "      <td>dprleanne</td>\n",
       "      <td>lunes nanaman bukas #academicbreaknow</td>\n",
       "      <td>lunes nanaman bukas #academicbreaknow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-27 09:02:47+00:00</td>\n",
       "      <td>ericakieraa</td>\n",
       "      <td>#AcademicBreakNow gusto q na tapusin Wednesday</td>\n",
       "      <td>#AcademicBreakNow gusto q na tapusin Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-25 13:30:42+00:00</td>\n",
       "      <td>qin_ina</td>\n",
       "      <td>super delay na ako sa tbw list ko #academicbre...</td>\n",
       "      <td>super delay na ako sa tbw list ko #academicbre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-11-24 15:46:35+00:00</td>\n",
       "      <td>zellyze</td>\n",
       "      <td>@angewwaa same beh üò≠ academicbreaknow!!!</td>\n",
       "      <td>same beh üò≠ academicbreaknow!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-11-24 15:40:42+00:00</td>\n",
       "      <td>louvri_</td>\n",
       "      <td>#AcademicBreakNow</td>\n",
       "      <td>#AcademicBreakNow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-11-24 12:46:22+00:00</td>\n",
       "      <td>_patreng_</td>\n",
       "      <td>#/academicbreaknow tsngina pagod na 'ko magpaypay</td>\n",
       "      <td>#/academicbreaknow tsngina pagod na 'ko magpaypay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-11-24 08:18:08+00:00</td>\n",
       "      <td>willowveewise</td>\n",
       "      <td>Pagoda ang accla, 4hours tulog gising 3:40am l...</td>\n",
       "      <td>Pagoda ang accla, 4hours tulog gising 3:40am l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-11-22 15:31:24+00:00</td>\n",
       "      <td>rielles_cart</td>\n",
       "      <td>sa letra ng p, putangina pagod na ko #academic...</td>\n",
       "      <td>sa letra ng p, putangina pagod na ko #academic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date           user  \\\n",
       "0   2022-11-29 04:15:58+00:00    maffyolfato   \n",
       "1   2022-11-27 12:18:49+00:00    researcheff   \n",
       "2   2022-11-27 10:30:42+00:00      dprleanne   \n",
       "3   2022-11-27 09:02:47+00:00    ericakieraa   \n",
       "4   2022-11-25 13:30:42+00:00        qin_ina   \n",
       "5   2022-11-24 15:46:35+00:00        zellyze   \n",
       "6   2022-11-24 15:40:42+00:00        louvri_   \n",
       "7   2022-11-24 12:46:22+00:00      _patreng_   \n",
       "8   2022-11-24 08:18:08+00:00  willowveewise   \n",
       "10  2022-11-22 15:31:24+00:00   rielles_cart   \n",
       "\n",
       "                                                 text  \\\n",
       "0                                   #academicbreaknow   \n",
       "1   Hello, #AcademicTwitter,\\n\\nüéôÔ∏è\"You need to hav...   \n",
       "2               lunes nanaman bukas #academicbreaknow   \n",
       "3      #AcademicBreakNow gusto q na tapusin Wednesday   \n",
       "4   super delay na ako sa tbw list ko #academicbre...   \n",
       "5            @angewwaa same beh üò≠ academicbreaknow!!!   \n",
       "6                                   #AcademicBreakNow   \n",
       "7   #/academicbreaknow tsngina pagod na 'ko magpaypay   \n",
       "8   Pagoda ang accla, 4hours tulog gising 3:40am l...   \n",
       "10  sa letra ng p, putangina pagod na ko #academic...   \n",
       "\n",
       "                                          tidy_tweets  \n",
       "0                                   #academicbreaknow  \n",
       "1   Hello, #AcademicTwitter,\\n\\nüéôÔ∏è\"You need to hav...  \n",
       "2               lunes nanaman bukas #academicbreaknow  \n",
       "3      #AcademicBreakNow gusto q na tapusin Wednesday  \n",
       "4   super delay na ako sa tbw list ko #academicbre...  \n",
       "5                      same beh üò≠ academicbreaknow!!!  \n",
       "6                                   #AcademicBreakNow  \n",
       "7   #/academicbreaknow tsngina pagod na 'ko magpaypay  \n",
       "8   Pagoda ang accla, 4hours tulog gising 3:40am l...  \n",
       "10  sa letra ng p, putangina pagod na ko #academic...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['tidy_tweets'] = np.vectorize(remove_pattern)(df2['text'], \"@[\\w]*\")\n",
    "\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-philip",
   "metadata": {},
   "source": [
    "###  Removing Punctuations, Numbers, and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "satisfied-raise",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-58216521fa57>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df2['tidy_tweets'] = df2['tidy_tweets'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
      "<ipython-input-10-58216521fa57>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['tidy_tweets'] = df2['tidy_tweets'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>tidy_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-29 04:15:58+00:00</td>\n",
       "      <td>maffyolfato</td>\n",
       "      <td>#academicbreaknow</td>\n",
       "      <td>#academicbreaknow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-27 12:18:49+00:00</td>\n",
       "      <td>researcheff</td>\n",
       "      <td>Hello, #AcademicTwitter,\\n\\nüéôÔ∏è\"You need to hav...</td>\n",
       "      <td>Hello  #AcademicTwitter      You need to have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-27 10:30:42+00:00</td>\n",
       "      <td>dprleanne</td>\n",
       "      <td>lunes nanaman bukas #academicbreaknow</td>\n",
       "      <td>lunes nanaman bukas #academicbreaknow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-27 09:02:47+00:00</td>\n",
       "      <td>ericakieraa</td>\n",
       "      <td>#AcademicBreakNow gusto q na tapusin Wednesday</td>\n",
       "      <td>#AcademicBreakNow gusto q na tapusin Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-25 13:30:42+00:00</td>\n",
       "      <td>qin_ina</td>\n",
       "      <td>super delay na ako sa tbw list ko #academicbre...</td>\n",
       "      <td>super delay na ako sa tbw list ko #academicbre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-11-24 15:46:35+00:00</td>\n",
       "      <td>zellyze</td>\n",
       "      <td>@angewwaa same beh üò≠ academicbreaknow!!!</td>\n",
       "      <td>same beh   academicbreaknow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-11-24 15:40:42+00:00</td>\n",
       "      <td>louvri_</td>\n",
       "      <td>#AcademicBreakNow</td>\n",
       "      <td>#AcademicBreakNow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-11-24 12:46:22+00:00</td>\n",
       "      <td>_patreng_</td>\n",
       "      <td>#/academicbreaknow tsngina pagod na 'ko magpaypay</td>\n",
       "      <td># academicbreaknow tsngina pagod na  ko magpaypay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-11-24 08:18:08+00:00</td>\n",
       "      <td>willowveewise</td>\n",
       "      <td>Pagoda ang accla, 4hours tulog gising 3:40am l...</td>\n",
       "      <td>Pagoda ang accla   hours tulog gising     am l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-11-22 15:31:24+00:00</td>\n",
       "      <td>rielles_cart</td>\n",
       "      <td>sa letra ng p, putangina pagod na ko #academic...</td>\n",
       "      <td>sa letra ng p  putangina pagod na ko #academic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date           user  \\\n",
       "0   2022-11-29 04:15:58+00:00    maffyolfato   \n",
       "1   2022-11-27 12:18:49+00:00    researcheff   \n",
       "2   2022-11-27 10:30:42+00:00      dprleanne   \n",
       "3   2022-11-27 09:02:47+00:00    ericakieraa   \n",
       "4   2022-11-25 13:30:42+00:00        qin_ina   \n",
       "5   2022-11-24 15:46:35+00:00        zellyze   \n",
       "6   2022-11-24 15:40:42+00:00        louvri_   \n",
       "7   2022-11-24 12:46:22+00:00      _patreng_   \n",
       "8   2022-11-24 08:18:08+00:00  willowveewise   \n",
       "10  2022-11-22 15:31:24+00:00   rielles_cart   \n",
       "\n",
       "                                                 text  \\\n",
       "0                                   #academicbreaknow   \n",
       "1   Hello, #AcademicTwitter,\\n\\nüéôÔ∏è\"You need to hav...   \n",
       "2               lunes nanaman bukas #academicbreaknow   \n",
       "3      #AcademicBreakNow gusto q na tapusin Wednesday   \n",
       "4   super delay na ako sa tbw list ko #academicbre...   \n",
       "5            @angewwaa same beh üò≠ academicbreaknow!!!   \n",
       "6                                   #AcademicBreakNow   \n",
       "7   #/academicbreaknow tsngina pagod na 'ko magpaypay   \n",
       "8   Pagoda ang accla, 4hours tulog gising 3:40am l...   \n",
       "10  sa letra ng p, putangina pagod na ko #academic...   \n",
       "\n",
       "                                          tidy_tweets  \n",
       "0                                   #academicbreaknow  \n",
       "1   Hello  #AcademicTwitter      You need to have ...  \n",
       "2               lunes nanaman bukas #academicbreaknow  \n",
       "3      #AcademicBreakNow gusto q na tapusin Wednesday  \n",
       "4   super delay na ako sa tbw list ko #academicbre...  \n",
       "5                      same beh   academicbreaknow     \n",
       "6                                   #AcademicBreakNow  \n",
       "7   # academicbreaknow tsngina pagod na  ko magpaypay  \n",
       "8   Pagoda ang accla   hours tulog gising     am l...  \n",
       "10  sa letra ng p  putangina pagod na ko #academic...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['tidy_tweets'] = df2['tidy_tweets'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-mechanics",
   "metadata": {},
   "source": [
    "### Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "returning-aggregate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-e81f0b0f3605>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['tidy_tweets'] = cleaned_tweets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>tidy_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-29 04:15:58+00:00</td>\n",
       "      <td>maffyolfato</td>\n",
       "      <td>#academicbreaknow</td>\n",
       "      <td>#academicbreaknow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-27 12:18:49+00:00</td>\n",
       "      <td>researcheff</td>\n",
       "      <td>Hello, #AcademicTwitter,\\n\\nüéôÔ∏è\"You need to hav...</td>\n",
       "      <td>Hello #AcademicTwitter You need to have a rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-27 10:30:42+00:00</td>\n",
       "      <td>dprleanne</td>\n",
       "      <td>lunes nanaman bukas #academicbreaknow</td>\n",
       "      <td>lunes nanaman bukas #academicbreaknow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-27 09:02:47+00:00</td>\n",
       "      <td>ericakieraa</td>\n",
       "      <td>#AcademicBreakNow gusto q na tapusin Wednesday</td>\n",
       "      <td>#AcademicBreakNow gusto q na tapusin Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-25 13:30:42+00:00</td>\n",
       "      <td>qin_ina</td>\n",
       "      <td>super delay na ako sa tbw list ko #academicbre...</td>\n",
       "      <td>super delay na ako sa tbw list ko #academicbre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-11-24 15:46:35+00:00</td>\n",
       "      <td>zellyze</td>\n",
       "      <td>@angewwaa same beh üò≠ academicbreaknow!!!</td>\n",
       "      <td>same beh academicbreaknow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-11-24 15:40:42+00:00</td>\n",
       "      <td>louvri_</td>\n",
       "      <td>#AcademicBreakNow</td>\n",
       "      <td>#AcademicBreakNow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-11-24 12:46:22+00:00</td>\n",
       "      <td>_patreng_</td>\n",
       "      <td>#/academicbreaknow tsngina pagod na 'ko magpaypay</td>\n",
       "      <td># academicbreaknow tsngina pagod na ko magpaypay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-11-24 08:18:08+00:00</td>\n",
       "      <td>willowveewise</td>\n",
       "      <td>Pagoda ang accla, 4hours tulog gising 3:40am l...</td>\n",
       "      <td>Pagoda ang accla hours tulog gising am lecture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-11-22 15:31:24+00:00</td>\n",
       "      <td>rielles_cart</td>\n",
       "      <td>sa letra ng p, putangina pagod na ko #academic...</td>\n",
       "      <td>sa letra ng p putangina pagod na ko #academicb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date           user  \\\n",
       "0   2022-11-29 04:15:58+00:00    maffyolfato   \n",
       "1   2022-11-27 12:18:49+00:00    researcheff   \n",
       "2   2022-11-27 10:30:42+00:00      dprleanne   \n",
       "3   2022-11-27 09:02:47+00:00    ericakieraa   \n",
       "4   2022-11-25 13:30:42+00:00        qin_ina   \n",
       "5   2022-11-24 15:46:35+00:00        zellyze   \n",
       "6   2022-11-24 15:40:42+00:00        louvri_   \n",
       "7   2022-11-24 12:46:22+00:00      _patreng_   \n",
       "8   2022-11-24 08:18:08+00:00  willowveewise   \n",
       "10  2022-11-22 15:31:24+00:00   rielles_cart   \n",
       "\n",
       "                                                 text  \\\n",
       "0                                   #academicbreaknow   \n",
       "1   Hello, #AcademicTwitter,\\n\\nüéôÔ∏è\"You need to hav...   \n",
       "2               lunes nanaman bukas #academicbreaknow   \n",
       "3      #AcademicBreakNow gusto q na tapusin Wednesday   \n",
       "4   super delay na ako sa tbw list ko #academicbre...   \n",
       "5            @angewwaa same beh üò≠ academicbreaknow!!!   \n",
       "6                                   #AcademicBreakNow   \n",
       "7   #/academicbreaknow tsngina pagod na 'ko magpaypay   \n",
       "8   Pagoda ang accla, 4hours tulog gising 3:40am l...   \n",
       "10  sa letra ng p, putangina pagod na ko #academic...   \n",
       "\n",
       "                                          tidy_tweets  \n",
       "0                                   #academicbreaknow  \n",
       "1   Hello #AcademicTwitter You need to have a rest...  \n",
       "2               lunes nanaman bukas #academicbreaknow  \n",
       "3      #AcademicBreakNow gusto q na tapusin Wednesday  \n",
       "4   super delay na ako sa tbw list ko #academicbre...  \n",
       "5                           same beh academicbreaknow  \n",
       "6                                   #AcademicBreakNow  \n",
       "7    # academicbreaknow tsngina pagod na ko magpaypay  \n",
       "8   Pagoda ang accla hours tulog gising am lecture...  \n",
       "10  sa letra ng p putangina pagod na ko #academicb...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tweets = []\n",
    "\n",
    "for index, row in df2.iterrows():\n",
    "    # Here we are filtering out all the words that contains link\n",
    "    words_without_links = [word for word in row.tidy_tweets.split() if 'http' not in word]\n",
    "    cleaned_tweets.append(' '.join(words_without_links))\n",
    "\n",
    "df2['tidy_tweets'] = cleaned_tweets\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-midwest",
   "metadata": {},
   "source": [
    "### Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ultimate-addiction",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tidy_tweets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tidy_tweets'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5092e6b31bd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mclean_tweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"#[A-Za-z0-9_]+\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclean_tweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtweets_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tidy_tweets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tidy_tweets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tidy_tweets'"
     ]
    }
   ],
   "source": [
    "def clean_char(text):\n",
    "    clean_tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "    clean_tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", clean_tweet)\n",
    "    return clean_tweet\n",
    "tweets_df['tidy_tweets']=dataset['tidy_tweets'].apply(clean_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-eleven",
   "metadata": {},
   "source": [
    "### Remove rows with empty texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = df2[df2['tidy_tweets']!='']\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-andrew",
   "metadata": {},
   "source": [
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.drop_duplicates(subset=['tidy_tweets'], keep='first')\n",
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-oasis",
   "metadata": {},
   "source": [
    "### Reset Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.reset_index(drop=True)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-occasions",
   "metadata": {},
   "source": [
    "### Remove special characters again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['absolute_tidy_tweets'] = tweets_df['tidy_tweets'].str.replace(\"[^a-zA-Z# ]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-russell",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-functionality",
   "metadata": {},
   "source": [
    "# Remove English and Filipino Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_eng = nltk.corpus.stopwords.words('english')\n",
    "stopword_fil = [\"akin\",\"aking\",\"ako\",\"alin\",\"am\",\"amin\",\"aming\",\"ang\",\"ano\",\"anumang\",\"apat\",\"at\",\"atin\",\"ating\",\"ay\",\"bababa\",\"bago\",\"bakit\",\"bawat\",\"bilang\",\"dahil\",\"dalawa\",\"dapat\",\"din\",\"dito\",\"doon\",\"gagawin\",\"gayunman\",\"ginagawa\",\"ginawa\",\"ginawang\",\"gumawa\",\"gusto\",\"habang\",\"hanggang\",\"hindi\",\"huwag\",\"iba\",\"ibaba\",\"ibabaw\",\"ibig\",\"ikaw\",\"ilagay\",\"ilalim\",\"ilan\",\"inyong\",\"isa\",\"isang\",\"itaas\",\"ito\",\"iyo\",\"iyon\",\"iyong\",\"ka\",\"kahit\",\"kailangan\",\"kailanman\",\"kami\",\"kanila\",\"kanilang\",\"kanino\",\"kanya\",\"kanyang\",\"kapag\",\"kapwa\",\"karamihan\",\"katiyakan\",\"katulad\",\"kaya\",\"kaysa\",\"ko\",\"kong\",\"kulang\",\"kumuha\",\"kung\",\"laban\",\"lahat\",\"lamang\",\"likod\",\"lima\",\"maaari\",\"maaaring\",\"maging\",\"mahusay\",\"makita\",\"marami\",\"marapat\",\"masyado\",\"may\",\"mayroon\",\"mga\",\"minsan\",\"mismo\",\"mula\",\"muli\",\"na\",\"nabanggit\",\"naging\",\"nagkaroon\",\"nais\",\"nakita\",\"namin\",\"napaka\",\"narito\",\"nasaan\",\"ng\",\"ngayon\",\"ni\",\"nila\",\"nilang\",\"nito\",\"niya\",\"niyang\",\"noon\",\"o\",\"pa\",\"paano\",\"pababa\",\"paggawa\",\"pagitan\",\"pagkakaroon\",\"pagkatapos\",\"palabas\",\"pamamagitan\",\"panahon\",\"pangalawa\",\"para\",\"paraan\",\"pareho\",\"pataas\",\"pero\",\"pumunta\",\"pumupunta\",\"sa\",\"saan\",\"sabi\",\"sabihin\",\"sarili\",\"sila\",\"sino\",\"siya\",\"tatlo\",\"tayo\",\"tulad\",\"tungkol\",\"una\",\"walang\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopword_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopword_fil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-surge",
   "metadata": {},
   "source": [
    "### Remove english stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_eng]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_eng]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "tweets_df['stopped_tweets']=tweets_df['absolute_tidy_tweets'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-trash",
   "metadata": {},
   "source": [
    "### Remove Filipino Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_fil]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_fil]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text.lower()\n",
    "\n",
    "tweets_df['stopped_tweets']=tweets_df['stopped_tweets'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-african",
   "metadata": {},
   "source": [
    "## export to csv for backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('02_Data_wo_Stopwords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-warrior",
   "metadata": {},
   "source": [
    "## Label Sentiments Automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-terminal",
   "metadata": {},
   "source": [
    "Compare Textblob and NLTK Sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "def fetch_sentiment_using_textblob(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return 'pos' if analysis.sentiment.polarity >= 0 else 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_using_textblob = tweets_df.text.apply(lambda tweet: fetch_sentiment_using_textblob(tweet))\n",
    "tweets_df['sentiment'] = sentiments_using_textblob\n",
    "pd.DataFrame(sentiments_using_textblob.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def fetch_sentiment_using_SIA(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    polarity_scores = sid.polarity_scores(text)\n",
    "    return 'neg' if polarity_scores['neg'] > polarity_scores['pos'] else 'pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_using_SIA = tweets_df.text.apply(lambda tweet: fetch_sentiment_using_SIA(tweet))\n",
    "tweets_df['sentiment'] = sentiments_using_SIA\n",
    "pd.DataFrame(sentiments_using_SIA.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.loc[tweets_df.sentiment == 'neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-store",
   "metadata": {},
   "source": [
    "### Export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('03_Data_with_sentiment.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3196968d684371006099b3d55edeef8ed90365227a30deaef86e5d4aa8519be0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
